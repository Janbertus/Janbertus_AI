# Ollama Client (Node.js)

# ğŸ¤– Janbertus AI

Ein minimalistisches Node.js-Tool, um mit einem lokal laufenden Ollama KI-Modell wie `gpt-oss:20b` zu kommunizieren.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Node.js](https://img.shields.io/badge/node.js-v18+-green.svg)
![Express](https://img.shields.io/badge/express-v5.1.0-blue.svg)

## ğŸŒŸ Features

- **ğŸ–¥ï¸ Moderne Web-OberflÃ¤che**: Saubere, responsive Chat-UI
- **ğŸ§  Lokale KI-Integration**: Direkte Verbindung zu Ollama-Modellen
- **ğŸ“ Markdown-Support**: Formatierte Antworten mit Code-Highlighting
- **âš¡ Real-time Chat**: Schnelle Antworten ohne VerzÃ¶gerung
- **ğŸ¨ Ansprechendes Design**: Moderne CSS-Styles mit Animationen
- **ğŸ”’ 100% Lokal**: Keine externen APIs, komplette PrivatsphÃ¤re

## ğŸš€ Quick Start

### Voraussetzungen

- [Node.js](https://nodejs.org/) (v18 oder hÃ¶her)
- [Ollama](https://ollama.ai/) mit installiertem `gpt-oss:20b` Modell

### Installation

1. **Repository klonen oder herunterladen**
   ```bash
   git clone https://github.com/Janbertus/Janbertus_AI.git
   cd Janbertus_AI
   ```

2. **Dependencies installieren**
   ```bash
   npm install
   ```

3. **Ollama starten** (in separatem Terminal)
   ```bash
   ollama serve
   ```

4. **GPT-OSS Modell laden** (falls noch nicht geschehen)
   ```bash
   ollama pull gpt-oss:20b
   ```

5. **Server starten**
   ```bash
   npm start
   ```

6. **Browser Ã¶ffnen**: http://localhost:3000

## ğŸ“ Projektstruktur

```
Janbertus_AI/
â”œâ”€â”€ ğŸ“„ server.js          # Express.js Backend
â”œâ”€â”€ ğŸ“„ package.json       # NPM Konfiguration
â”œâ”€â”€ ğŸ“„ README.md          # Diese Datei
â””â”€â”€ public/               # Frontend-Dateien
    â”œâ”€â”€ ğŸ“„ index.html     # Haupt-HTML
    â”œâ”€â”€ ğŸ“„ script.js      # JavaScript Logic
    â””â”€â”€ ğŸ¨ style.css      # Styling
```

## ğŸ› ï¸ Technologie-Stack

- **Backend**: Node.js + Express.js
- **Frontend**: Vanilla JavaScript + HTML5 + CSS3
- **KI-Integration**: Ollama API
- **Markdown**: Marked.js fÃ¼r formatierte Ausgabe

## âš™ï¸ Konfiguration

### Modell Ã¤ndern
In `server.js` kannst du das Ollama-Modell anpassen:

```javascript
const response = await axios.post('http://localhost:11434/api/chat', {
    model: 'dein-gewÃ¼nschtes-modell',  // z.B. 'llama2', 'mistral'
    // ...
});
```

### Port Ã¤ndern
```javascript
const PORT = 3000; // Ã„ndere hier den Port
```

## ğŸ¯ Verwendung

1. **Frage eingeben**: Tippe deine Frage in das Eingabefeld
2. **Enter drÃ¼cken**: Oder auf "Senden" klicken
3. **Antwort erhalten**: Die KI antwortet in formatiertem Markdown
4. **Chat-Verlauf**: Alle Nachrichten bleiben im Chat sichtbar

## ğŸ› Troubleshooting

### HÃ¤ufige Probleme

**Server startet nicht:**
- PrÃ¼fe ob Port 3000 frei ist: `netstat -ano | findstr :3000`
- Andere Ports mit `PORT=4000 npm start` verwenden

**Keine KI-Antworten:**
- Ollama lÃ¤uft: `curl http://localhost:11434/api/tags`
- Modell verfÃ¼gbar: `ollama list`

**Verbindungsfehler:**
- Firewall/Antivirus prÃ¼fen
- Localhost-Zugriff erlauben

## ğŸš§ Entwicklung

### Features hinzufÃ¼gen
```bash
# Development Server mit Auto-Reload
npm install -g nodemon
nodemon server.js
```

### Geplante Features
- [ ] ğŸ’¾ Chat-History speichern
- [ ] ğŸ¨ Theme-Wechsler (Hell/Dunkel)
- [ ] ğŸ“ Datei-Upload fÃ¼r Kontext
- [ ] ğŸ”§ Modell-Switcher in UI
- [ ] ğŸ“Š Token-Counter

## ğŸ“ API Endpoints

### POST `/api/ask`
Stellt eine Frage an das KI-Modell.

**Request:**
```json
{
  "question": "Was ist Quantencomputing?"
}
```

**Response:**
```json
{
  "answer": "# Quantencomputing\n\nQuantencomputing ist..."
}
```

## ğŸ¤ Contributing

1. Fork das Repository
2. Feature Branch erstellen: `git checkout -b feature/AmazingFeature`
3. Commit changes: `git commit -m 'Add AmazingFeature'`
4. Push to branch: `git push origin feature/AmazingFeature`
5. Pull Request Ã¶ffnen

## ğŸ“œ Lizenz

Dieses Projekt steht unter der MIT-Lizenz. Siehe `LICENSE` fÃ¼r Details.

## ğŸ™ Credits

- **Ollama Team** - FÃ¼r das fantastische lokale KI-Framework
- **Express.js** - FÃ¼r das robuste Web-Framework
- **Marked.js** - FÃ¼r Markdown-Parsing

---

**Erstellt mit â¤ï¸ von Janbertus**

*Lokale KI, maximale Kontrolle!* âœ¨

## ğŸ§  Was ist Ollama?

[Ollama](https://ollama.com) ermÃ¶glicht es, Sprachmodelle lokal auszufÃ¼hren. Dieses Tool stellt eine einfache Frage-Antwort-Interaktion mit dem Modell her.

## ğŸš€ Setup

### Voraussetzungen

- Node.js (empfohlen: LTS-Version)
- Ollama installiert und Modell geladen:
  ```bash
  ollama pull gpt-oss:20b
